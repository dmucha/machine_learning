{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7-tipc9LwYo"
      },
      "source": [
        "# Embedded AI&ML Test Cases\n",
        "\n",
        "## MNIST Handwritten Digits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Using TensorFlow version\", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "print(\"Using Keras version     \", keras.__version__)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,  Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "%matplotlib inline\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\""
      ],
      "metadata": {
        "id": "p4SkGQhXS9E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yETmvTJXLwYy"
      },
      "source": [
        "## Download the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "width = X_train.shape[2]\n",
        "height = X_train.shape[1]"
      ],
      "metadata": {
        "id": "uWM_1ddoUKzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf1HyrX9LwY4"
      },
      "source": [
        "## Start Processing the Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNaXc1ZfLwY5"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.utils import np_utils\n",
        "\n",
        "def decode_one_hot(y):\n",
        "    y_classes = [np.argmax(yi, axis=None, out=None) for yi in y]\n",
        "    return y_classes\n",
        "\n",
        "# Use one-hot coding to code the labels into binary.  The neural net needs this.\n",
        "y_train = y_train.astype(int)\n",
        "y_train = np_utils.to_categorical(y_train) # binarize the integer class ID's\n",
        "\n",
        "y_test = y_test.astype(int)\n",
        "y_test = np_utils.to_categorical(y_test) # binarize the integer class ID's\n",
        "\n",
        "# Scale the X parameters for processing by the neural net\n",
        "X_train = X_train.reshape(-1, 1)\n",
        "X_train = MinMaxScaler(feature_range=(-1, 1)).fit_transform(X_train)\n",
        "\n",
        "X_test = X_test.reshape(-1, 1)\n",
        "X_test = MinMaxScaler(feature_range=(-1, 1)).fit_transform(X_test)\n",
        "\n",
        "# Reshape X correctly to (images, width, height)\n",
        "X_train = X_train.reshape(-1, width, height)\n",
        "X_test = X_test.reshape(-1, width, height)\n",
        "\n",
        "print (\"X_train shape:\", X_train.shape)\n",
        "print (\"y_train shape:\", y_train.shape)\n",
        "\n",
        "print (\"X_test shape:\", X_test.shape)\n",
        "print (\"y_test  shape:\", y_test.shape)\n",
        "\n",
        "# Split half of the test set into a validation set\n",
        "split = int(X_test.shape[0] / 2)\n",
        "\n",
        "X_val = X_test[split:]\n",
        "y_val = y_test[split:]\n",
        "\n",
        "X_test = X_test[0:split]\n",
        "y_test = y_test[0:split]\n",
        "\n",
        "# Reshape X_train, X_test, X_val to (images, width, height)\n",
        "X_train = X_train.reshape(X_train.shape[0], width, height, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], width, height, 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], width, height, 1)\n",
        "\n",
        "# Input shape for the first layer of the Neural Net\n",
        "input_shape = (width, height, 1)\n",
        "\n",
        "print(\"X_train shape  X_test shape  X_val shape\")\n",
        "print(X_train.shape, X_test.shape, X_val.shape)\n",
        "print(\"y_train shape  y_test shape  y_val shape\")\n",
        "print(y_train.shape, y_test.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmQn-dRiLwY7"
      },
      "source": [
        "## Display the images\n",
        "\n",
        "Here's how we can display the images if that is interesting.  The example below displays the first 100ish images in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu1Y9uimLwY9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "\n",
        "def display_image(array):\n",
        "    %pylab inline    \n",
        "    img = Image.fromarray(array)\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "# Convert images to int8 and scale for plotting by imshow\n",
        "X_plot = (X_train * 255).astype(np.int8)\n",
        "X_plot = X_plot.reshape(-1, width, height)\n",
        "print (\"X_plot Shape:\", X_plot.shape)\n",
        "\n",
        "dec_y = decode_one_hot(y_train)\n",
        "for i,d in enumerate(X_plot):\n",
        "    display_image(d)\n",
        "    print(dec_y[i])\n",
        "    if i == 100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idExROuELwZA"
      },
      "source": [
        "## Define and load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bBvpaLoJLwZB"
      },
      "outputs": [],
      "source": [
        "learning_rate = .001\n",
        "\n",
        "## Define the Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(10, kernel_size=(5,5), strides=(1,1), padding='same', activation=\"tanh\", input_shape=input_shape))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation=\"tanh\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['categorical_accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9BkCCcmLwZC"
      },
      "source": [
        "## Train the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZufxhqjtLwZC"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 20\n",
        "\n",
        "## Fit the Model\n",
        "hist = model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          shuffle=False,\n",
        "          validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ofFhoSLwZD"
      },
      "source": [
        "## Score the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "oj6viqKRLwZD"
      },
      "outputs": [],
      "source": [
        "## Evaluate the Model\n",
        "\n",
        "score = model.evaluate(X_test, y_test)\n",
        "score_val = model.evaluate(X_val, y_val)\n",
        "\n",
        "yhat = np.argmax(model.predict(X_test), axis=-1)\n",
        "yhat_val = np.argmax(model.predict(X_val), axis=-1)\n",
        "\n",
        "y_test1 = decode_one_hot(y_test)\n",
        "y_val1 = decode_one_hot(y_val)\n",
        "\n",
        "print()\n",
        "print(metrics.classification_report(y_test1, yhat))\n",
        "print(metrics.confusion_matrix(y_test1,yhat))\n",
        "print(\"Testing Loss:\", score[0])\n",
        "print(\"Testing Accuracy:\", score[1])\n",
        "\n",
        "print()\n",
        "print(metrics.classification_report(y_val1, yhat_val))\n",
        "print(metrics.confusion_matrix(y_val1, yhat_val))\n",
        "print(\"Validation Loss:\", score_val[0])\n",
        "print(\"Validation Accuracy:\", score_val[1])\n",
        "\n",
        "\n",
        "## Plot the accuracy vs. validation accuracy\n",
        "\n",
        "epoch_list = list(range(1, len(hist.history['categorical_accuracy']) + 1))\n",
        "plt.plot(epoch_list, hist.history['categorical_accuracy'], epoch_list, hist.history['val_categorical_accuracy'])\n",
        "plt.legend((\"Training Accuracy\", \"Validation Accuracy\"))\n",
        "plt.show()\n",
        "plt.plot(epoch_list, hist.history['loss'], epoch_list, hist.history['val_loss'])\n",
        "plt.legend((\"Training Loss\", \"Validation Loss\"))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "MNIST_demo_2021_1.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}